#!/usr/bin/env python

"""
This script enables users to retrieve information from BreachDB.
It is important to note that this script retrieves matching emails/passwords and saves these credentials in two different files.
The advantage of this is that the credentials can be easily used with Burp Intruder in *Pitchfork* mode, since the payloads are related.
As a consequence, while it gets rid of duplicated credentials, the output file containing emails will have duplicates,
if multiple passwords for the same user were found.
"""

from __future__ import print_function
from contextlib import closing
from functools import partial
import sys, os, argparse, json, requests, logging
from multiprocessing import Pool as ThreadPool
from tqdm import tqdm
# only for performance comparisons
import time

__author__ = "secjey"
__copyright__ = "Copyright 2017"
__credits__ = [""]
__license__ = "GPLv3"
__version__ = "1.0.1"
__maintainer__ = "secjey"
__status__ = "Development"

MAX_THREADS = 3
START_INDEX = 1
MAX_RESULTS = 5000
NO_RESULT_SIZE = 100	# when there is no results in the DB, the HTTP body is smaller than this value (in bytes)
# format: /creds?domain=test.com or /creds?email=email@test.com
BREACHDB_URL = "http://DOMAIN.COM/creds?{}={}&start={}&end={}"
EMAIL_FILE = 'emails.txt'
PASSWORD_FILE = 'passwords.txt'
CREDS_FILE = 'creds.txt'
OUTPUT_DIR = '{}/output_breachdb'.format(os.path.dirname(os.path.abspath(__file__)))

class bcolors:
    """Defines some ANSI color codes."""
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    ITALIC = '\033[3m'
    UNDERLINE = '\033[4m'


def welcome():
    """Prints the welcome message at the start of the script."""
    print(bcolors.HEADER + """
    This tool enables users to retrieve information from BreachDB.
    By secjey - https://github.com/secjey
	""" + bcolors.ENDC)

def get_parser():
	"""Parses the arguments passed in the command line."""
	command_examples = """
EXAMPLES

Request a single email address:

	python request_breachdb email@test.com

Request a single domain:

	python request_breachdb domain.com

Request multiple email addresses/domains contained in data.txt and get maximum 10,000 results for each of them:

	python request_breachdb data.txt -v --max 10000
	"""
	parser = argparse.ArgumentParser(description='DESCRIPTION', epilog=command_examples, formatter_class=argparse.RawDescriptionHelpFormatter)
	parser.add_argument('item', help="email address or domain to look up. For multiple emails/domains, a file can be provided.")
	parser.add_argument('--output-emails', metavar="OUTPUT_FILE", help="path to the output file containing retrieved emails.", default=EMAIL_FILE)
	parser.add_argument('--output-passwords', metavar="OUTPUT_FILE", help="path to the output file containing retrieved passwords.", default=PASSWORD_FILE)
	parser.add_argument('--output-creds', metavar="OUTPUT_FILE", help="path to the output file containing retrieved emails & passwords.", default=CREDS_FILE)
	parser.add_argument('--start', type=int, metavar="START_INDEX", help="index in BreachDB from which to start looking.", default=START_INDEX)
	group = parser.add_mutually_exclusive_group()
	group.add_argument('--max', type=int, metavar="MAX_RESULTS", help="max number of results to retrieve from database.", default=MAX_RESULTS)
	group.add_argument('--all', action="store_true", help="retrieve all results from database.", default=False)
	parser.add_argument('--threads', type=int, metavar="MAX_THREADS", help="max number of threads to retrieve/process data.", default=MAX_THREADS)
	parser.add_argument("-v", "--verbose", help="increase output verbosity.", action="count", default=0)
	return parser

def validate_args(args):
	"""Validates input data before processing it."""
	if args.threads < 1:
		logging.warning(bcolors.WARNING + "[!] The threads value must be bigger than 0..." + bcolors.ENDC)
	elif args.start < 0:
		logging.warning(bcolors.WARNING + "[!] The start value must be bigger than 1..." + bcolors.ENDC)
	elif args.max < 1:
		logging.warning(bcolors.WARNING + "[!] The max value must be bigger than 1..." + bcolors.ENDC)
	else:
		return
	sys.exit()

def read(input_file):
	"""Reads data from input_file."""
	items = list()
	with open(input_file, 'r') as data_file:
		for item in data_file:
			items.append(item.strip('\n'))
	return items

def write(output_file, data_from_list):
	"""Writes data_from_list into output_file."""
	with open(output_file, 'w') as data_file:
		data_file.write('\n'.join(data_from_list).encode('utf8'))

def store_creds(output_emails, output_passwords, output_creds, item, creds):
	"""Stores credentials into files."""
	item_type = get_data_type(item)
	if creds:
		if item_type != "email":
			# save into files
			write("{}/{}_{}".format(OUTPUT_DIR, item, output_emails), creds[0])
			write("{}/{}_{}".format(OUTPUT_DIR, item, output_passwords), creds[1])
			write("{}/{}_{}".format(OUTPUT_DIR, item, output_creds), list(map(lambda x,y: x + ':' + y, creds[0], creds[1])))
			return True
		else:
			return True
	return False

	
def retrieve_json(url, item, item_type, get_all):
	"""Gets JSON data from url."""
	# TODO - implement retrieve all results feature here
	json_raw_data = ""
	# get remote data as stream
	with closing(requests.get(url, stream=True)) as req:
		# total size in bytes
		total_size = int(req.headers['Content-Length'])
		chunk_size = 100*1024
		# if no result has been found in the BreachDB, the json data will be smaller than NO_RESULT_SIZE bytes
		# only display the progress bar for domain lookup and if there exist results
		print_progress_bar = True if (logging.getLogger(__name__).getEffectiveLevel() <= logging.INFO and total_size > NO_RESULT_SIZE and item_type == "domain") else False
		if print_progress_bar:	
			progress_bar = tqdm(total=total_size, unit='B', unit_scale=True, desc=item)
		for chunk in req.iter_content(chunk_size):
			if print_progress_bar:
				progress_bar.update(len(chunk))
			json_raw_data += chunk
	return json.loads(json_raw_data)["results"]

def forge_url(item, item_type, start, end):
	"""Forges the URL based on the provided parameters."""
	return BREACHDB_URL.format(item_type, item, start, end)

def get_data_type(item):
	"""Checks wether item is an email address or a domain."""
	return "email" if "@" in item else "domain"

def process_data(item, item_type, json_data):
	"""Processes the JSON data that has been retrieved."""
	sources, credentials = list(), list()
	for result in json_data:
		# append tuple of (email,password)
		credentials.append((result["email"],result["password"]))
		sources.append(result["source"])

	if credentials:
		# transform list of tuples into chained lists in the form [[email1,email2],[password1,password2]]
		# and get rid of duplicates (with set)
		credentials = map(list, zip(*(list(set(credentials)))))

		# if the item is an email address, only print the password(s) to stdout
		if item_type == "email":
			print(bcolors.OKGREEN + "[+] {}: {} ".format(item, ", ".join(credentials[1]))\
				+ bcolors.OKBLUE + "(data source: {})\n".format(", ".join(list(set(sources)))) + bcolors.ENDC)
			return (item,credentials)
		else:
			logging.info(bcolors.OKBLUE + "[-] Data source for {}: {}\n".format(item, ", ".join(list(set(sources)))) + bcolors.ENDC)
			return (item,credentials)

	else:
		logging.info(bcolors.WARNING + "[!] No data corresponding to {} was found in BreachDB...\n".format(item) + bcolors.ENDC)
		return (item,None)

def multi_lookup(items, start, end, threads, get_all):
	"""Starts a pool of threads to look multiple items up."""
	p = ThreadPool(threads)
	creds_by_item = p.map(partial(lookup, start=start, end=end, threads=threads, get_all=get_all), items)
	p.close()
	p.join()
	return creds_by_item

def lookup(item, start, end, threads, get_all):
	"""Looks item up in the BreachDB and processes it."""
	item_type = get_data_type(item)
	url = forge_url(item, item_type, start, end)
	json_data = retrieve_json(url, item, item_type, get_all)
	return process_data(item, item_type, json_data)
		
def main():
	welcome()
	parser = get_parser()
	args = parser.parse_args()
	validate_args(args)
	
	# choose logging level based on user-supplied param (-v, -vv)
	levels = [logging.WARNING, logging.INFO, logging.DEBUG]
	level = levels[min(len(levels)-1,args.verbose)]
	logging.basicConfig(level=level, format="%(message)s")

	start_time = time.clock()
	found = [False]

	# create an output dir to store the results
	if not os.path.exists(OUTPUT_DIR):
		os.makedirs(OUTPUT_DIR)
	
	# if the input item is a file, read it and start multiple lookups
	if os.path.isfile(args.item):
		items = read(args.item)
		found = [False] * len(items)
		creds_by_item = multi_lookup(items, args.start, args.start + args.max-1, args.threads, args.all)

		for i,(item, creds) in enumerate(creds_by_item):
			found[i] = store_creds(args.output_emails, args.output_passwords, args.output_creds, item, creds)

	else:	
		creds = lookup(args.item, args.start, args.start + args.max-1, args.threads, args.all)[1]
		found[0] = store_creds(args.output_emails, args.output_passwords, args.output_creds, args.item, creds)

	# remove the output dir if no result has been found
	if not os.listdir(OUTPUT_DIR):
		os.rmdir(OUTPUT_DIR)
	
	logging.info(bcolors.OKBLUE + "[-] Data has been found and retrieved" + bcolors.ENDC) if True in found else logging.info(bcolors.WARNING + "[!] No data has been found" + bcolors.ENDC)
	logging.info(bcolors.OKBLUE + "[-] Processing time: {}".format(time.clock() - start_time) + bcolors.ENDC)

if __name__ == '__main__':
	main()
